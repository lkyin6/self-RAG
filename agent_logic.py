# agent_logic.py
import json
import re
import time
from collections import Counter
from typing import List, Dict, Any, Optional

# LlamaIndex æ ¸å¿ƒç»„ä»¶
from llama_index.llms.ollama import Ollama
from llama_index.core import Settings

# =========================================================
# 1. LLM åˆå§‹åŒ– (å¸¦å®¹é”™)
# =========================================================
try:
    mid_llm = Ollama(model="qwen2.5:1.5b", request_timeout=120.0)
except Exception as e:
    print(f"Warning: Failed to init qwen2.5:1.5b ({e}), using default Settings.llm")
    mid_llm = Settings.llm

try:
    fast_llm = Ollama(model="qwen2.5:0.5b", request_timeout=60.0)
except Exception:
    fast_llm = mid_llm

# =========================================================
# 2. å·¥å…·å‡½æ•°
# =========================================================
def _strip_code_fence(text: str) -> str:
    text = str(text).strip()
    text = re.sub(r"```json", "", text, flags=re.IGNORECASE)
    text = re.sub(r"```", "", text)
    return text.strip()

def clean_json_obj(text: str) -> str:
    text = _strip_code_fence(text)
    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1 and end > start:
        return text[start : end + 1]
    return "{}"

# =========================================================
# 3. è¯Šæ–­ä¸Žè¯„ä¼°é€»è¾‘
# =========================================================
def diagnose_error(query: str, context_list: List[str], model_answer: str) -> Dict[str, Any]:
    context_text = "\n".join(context_list) if context_list else "(empty)"
    prompt = f"""
è¯·åˆ¤æ–­åŸºäºŽã€æ£€ç´¢ç‰‡æ®µã€‘ç”Ÿæˆçš„ã€AIå›žç­”ã€‘æ˜¯å¦å­˜åœ¨ä¸¥é‡é”™è¯¯ã€‚
è¾“å…¥ï¼š
- é—®ï¼š{query}
- ç­”ï¼š{model_answer}
- ç‰‡æ®µï¼š{context_text[:500]}

è¯·è¿”å›ž JSON æ ¼å¼ï¼š
{{
    "error_type": "GOOD" | "MISSING_CONTENT" | "NOISE" | "SEGMENTATION_FAULT",
    "reason": "ç®€çŸ­ç†ç”±"
}}
"""
    try:
        response = fast_llm.complete(prompt)
        obj = json.loads(clean_json_obj(str(response)))
        
        et = str(obj.get("error_type", "GOOD")).upper()
        if "MISSING" in et: et = "MISSING_CONTENT"
        elif "NOISE" in et: et = "NOISE"
        elif "SEGMENT" in et: et = "SEGMENTATION_FAULT"
        else: et = "GOOD"
        
        return {"error_type": et, "reason": obj.get("reason", "ok")}
    except Exception:
        return {"error_type": "GOOD", "reason": "Diagnosis Parse Error"}

def eval_context_relevance(query: str, contexts: List[str]) -> Dict[str, Any]:
    if not contexts:
        return {"relevance": 0, "reason": "No contexts retrieved"}
    
    prompt = f"""
è¯·å¯¹æ£€ç´¢ç‰‡æ®µä¸Žé—®é¢˜çš„ç›¸å…³æ€§æ‰“åˆ† (0-3)ã€‚
0: æ— å…³
1: ç•¥å¾®ç›¸å…³
2: ç›¸å…³
3: éžå¸¸ç›¸å…³/åŒ…å«ç­”æ¡ˆ

è¾“å…¥ï¼š
- é—®ï¼š{query}
- ç‰‡æ®µï¼š{contexts[0][:500]}

è¿”å›ž JSON: {{"relevance": 0, "reason": "..."}}
"""
    try:
        resp = mid_llm.complete(prompt)
        obj = json.loads(clean_json_obj(str(resp)))
        score = int(obj.get("relevance", 0))
        return {"relevance": score, "reason": obj.get("reason", "")}
    except Exception:
        return {"relevance": 0, "reason": "Relevance Parse Error"}

def rewrite_query(query: str) -> str:
    """å¦‚æžœæ£€ç´¢æ•ˆæžœä¸å¥½ï¼Œå°è¯•æ”¹å†™æŸ¥è¯¢"""
    try:
        prompt = f"è¯·æå–å…³é”®è¯å¹¶å°†ä»¥ä¸‹é—®é¢˜æ”¹å†™ä¸ºæ›´é€‚åˆæ£€ç´¢çš„å½¢å¼ï¼ˆåªè¾“å‡ºæ”¹å†™åŽçš„é—®é¢˜ï¼‰ï¼š\n{query}"
        return str(mid_llm.complete(prompt)).strip()
    except Exception:
        return query

# =========================================================
# 4. æ ¸å¿ƒï¼šç”Ÿæˆæµ‹è¯•é¢˜ (æ­£åˆ™åŒ¹é…ç‰ˆï¼Œæœ€å¼ºé²æ£’æ€§)
# =========================================================
def generate_test_set(doc_text: str) -> List[Dict[str, str]]:
    sample = doc_text[:1000]
    if len(doc_text) > 3000:
        sample += "\n...\n" + doc_text[len(doc_text)//2 : len(doc_text)//2 + 1000]
    
    prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œåˆ—å‡º 5 ä¸ªå€¼å¾—è€ƒæ ¸çš„å…³é”®é—®é¢˜ã€‚
ä¸è¦åŒ…å«ç­”æ¡ˆã€‚
æ¯è¡Œä¸€ä¸ªé—®é¢˜ã€‚

æ–‡æ¡£ç‰‡æ®µï¼š
{sample}
"""
    print("ðŸ¤– Agent æ­£åœ¨å°è¯•ç”Ÿæˆé—®é¢˜...")
    try:
        response = str(mid_llm.complete(prompt))
        
        # ç­–ç•¥1: æŠ“å– "1. é—®é¢˜" æˆ– "1ã€é—®é¢˜"
        pattern = r"\d+[\.\ã€]\s*(.*)"
        questions = re.findall(pattern, response)
        
        # ç­–ç•¥2: å¦‚æžœæ²¡æŠ“åˆ°ï¼ŒæŠ“å–å¸¦é—®å·çš„è¡Œ
        if not questions:
            questions = [l.strip() for l in response.split('\n') if ('?' in l or 'ï¼Ÿ' in l) and len(l.strip()) > 5]
            
        # ç­–ç•¥3: æš´åŠ›æŠ“å–é•¿å¥
        if not questions:
             questions = [l.strip() for l in response.split('\n') if len(l.strip()) > 8 and not l.strip().startswith(('-', '*', '#'))]

        final_set = []
        for q in questions:
            clean_q = re.sub(r"^[\-\*\#\>]\s*", "", q).strip()
            if len(clean_q) > 4:
                final_set.append({"question": clean_q, "standard_answer": "N/A"})
        
        if not final_set:
            print("âš ï¸ è‡ªåŠ¨ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é—®é¢˜")
            return [{"question": "æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ", "standard_answer": "N/A"}]
            
        print(f"âœ… æˆåŠŸç”Ÿæˆ {len(final_set[:5])} ä¸ªé—®é¢˜")
        return final_set[:5]

    except Exception as e:
        print(f"Generate Test Set Error: {e}")
        return [{"question": "æ–‡æ¡£ä¸»è¦è®²äº†ä»€ä¹ˆï¼Ÿ", "standard_answer": "N/A"}]

# =========================================================
# 5. æ ¸å¿ƒï¼šä¼˜åŒ–å¾ªçŽ¯ (å«æœ€ä½³é…ç½®å›žæº¯)
# =========================================================
def run_optimization_loop(rag_system, test_set, status_container, doc_hint: str = ""):
    current_config = rag_system.config
    logs = []
    
    # è®°å½•æœ€ä½³çŠ¶æ€
    best_score = -1.0
    best_config = rag_system.config.copy()

    for round_i in range(3):
        # ç¡®ä¿ç³»ç»Ÿé…ç½®ä¸Žå½“å‰ç­–ç•¥åŒæ­¥
        rag_system.config.update(current_config)
        
        status_container.markdown(f"**Round {round_i+1}** (Chunk={current_config['chunk_size']}, TopK={current_config['top_k']}...)")
        
        round_errors = []
        rel_scores = []
        empty_cnt = 0

        for idx, qa in enumerate(test_set):
            q = qa["question"]
            if round_i > 0 and "rewritten" in qa: 
                q = qa["rewritten"]

            # 1. æŸ¥è¯¢
            try:
                res = rag_system.query(q, llm=mid_llm, rerank_llm=fast_llm, return_debug=True)
            except Exception as e:
                res = {"answer": f"Error: {e}", "contexts": []}

            contexts = res.get("contexts", [])
            if not contexts: 
                empty_cnt += 1
            
            # 2. è¯„ä¼°
            rel_obj = eval_context_relevance(q, contexts)
            diag = diagnose_error(q, contexts, res.get("answer", ""))
            
            rel_scores.append(rel_obj["relevance"])
            if diag["error_type"] != "GOOD": 
                round_errors.append(diag["error_type"])

            # 3. æ—¥å¿—
            logs.append({
                "round": round_i+1, 
                "question_id": idx+1, 
                "config": current_config.copy(),
                "relevance": rel_obj, 
                "diagnosis": diag,
                "inputs": {
                    "question": q, 
                    "contexts": contexts, 
                    "model_answer": res.get("answer","")
                }
            })

        # --- å†³ç­– ---
        avg_rel = sum(rel_scores) / max(1, len(rel_scores))
        
        if avg_rel > best_score:
            best_score = avg_rel
            best_config = current_config.copy()
            status_container.success(f"ðŸ“ˆ Score: {avg_rel:.2f} (New Best!)")
        else:
            status_container.warning(f"ðŸ“‰ Score: {avg_rel:.2f} (Best was {best_score:.2f})")

        if not round_errors and avg_rel >= 2.5:
            break

        # --- è°ƒæ•´ ---
        if empty_cnt > 1:
            current_config["top_k"] += 2
            status_container.info("Action: TopK +2 (Fix Empty Context)")
        elif avg_rel < 1.5:
            current_config["retrieve_k_multiplier"] = min(10, current_config.get("retrieve_k_multiplier", 4) + 2)
            current_config["use_rerank"] = True
            for qa in test_set: qa["rewritten"] = rewrite_query(qa["question"])
            status_container.info("Action: Rewrite Query & Increase Multiplier")
        elif "NOISE" in round_errors:
            current_config["similarity_cutoff"] = 0.3
            status_container.info("Action: Enable Cutoff 0.3")
        elif "SEGMENTATION_FAULT" in round_errors:
            current_config["chunk_overlap"] += 50
            rag_system.build_index()
            status_container.info("Action: Increase Overlap")
        else:
            current_config["top_k"] += 1
            status_container.info("Action: TopK +1")

    return logs, best_config